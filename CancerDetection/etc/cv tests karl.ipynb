{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Load and Classify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karl\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=13.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "C:\\Users\\Karl\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "C:\\Users\\Karl\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "C:\\Users\\Karl\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "C:\\Users\\Karl\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=7.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'adaboost': {'bagging': {'KFold10': array([ 0.86666667,  0.85714286,  0.92307692,  0.91666667,  0.66666667,\n",
       "           0.91666667,  0.91666667,  1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold13': array([ 0.63636364,  0.9       ,  0.8       ,  0.9       ,  0.9       ,\n",
       "           0.7       ,  1.        ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.625     ]),\n",
       "   'KFold3': array([ 0.84090909,  0.92682927,  0.80487805]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.84      ,  0.95833333,  0.875     ]),\n",
       "   'KFold7': array([ 0.8       ,  0.94444444,  0.88888889,  0.88888889,  0.94444444,\n",
       "           1.        ,  0.88235294])},\n",
       "  'dtree': {'KFold10': array([ 0.8       ,  0.92857143,  1.        ,  0.91666667,  0.75      ,\n",
       "           0.91666667,  1.        ,  1.        ,  1.        ,  0.91666667]),\n",
       "   'KFold13': array([ 0.72727273,  0.8       ,  1.        ,  1.        ,  0.9       ,\n",
       "           0.5       ,  0.8       ,  0.9       ,  0.9       ,  1.        ,\n",
       "           1.        ,  1.        ,  0.625     ]),\n",
       "   'KFold3': array([ 0.90909091,  0.85365854,  0.87804878]),\n",
       "   'KFold5': array([ 0.81481481,  0.96153846,  0.84      ,  0.95833333,  0.91666667]),\n",
       "   'KFold7': array([ 0.75      ,  0.88888889,  0.94444444,  0.83333333,  0.88888889,\n",
       "           1.        ,  0.88235294])},\n",
       "  'randomforest': {'KFold10': array([ 0.8       ,  0.85714286,  0.92307692,  0.91666667,  0.75      ,\n",
       "           0.91666667,  1.        ,  1.        ,  1.        ,  0.83333333]),\n",
       "   'KFold13': array([ 0.72727273,  0.9       ,  0.8       ,  1.        ,  0.9       ,\n",
       "           0.7       ,  1.        ,  0.9       ,  0.9       ,  1.        ,\n",
       "           1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold3': array([ 0.90909091,  0.92682927,  0.70731707]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.84      ,  0.91666667,  0.91666667]),\n",
       "   'KFold7': array([ 0.8       ,  0.94444444,  0.94444444,  0.88888889,  0.94444444,\n",
       "           0.94117647,  0.88235294])}},\n",
       " 'bagging': {'bagging': {'KFold10': array([ 0.8       ,  0.92857143,  1.        ,  0.91666667,  0.75      ,\n",
       "           0.91666667,  0.91666667,  1.        ,  1.        ,  0.83333333]),\n",
       "   'KFold13': array([ 0.72727273,  0.8       ,  0.8       ,  1.        ,  0.8       ,\n",
       "           0.7       ,  0.9       ,  0.9       ,  0.9       ,  1.        ,\n",
       "           1.        ,  1.        ,  0.875     ]),\n",
       "   'KFold3': array([ 0.88636364,  0.90243902,  0.85365854]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.88      ,  1.        ,  0.91666667]),\n",
       "   'KFold7': array([ 0.8       ,  0.83333333,  0.94444444,  0.88888889,  0.88888889,\n",
       "           1.        ,  0.88235294])},\n",
       "  'dtree': {'KFold10': array([ 0.93333333,  0.85714286,  0.92307692,  0.91666667,  0.75      ,\n",
       "           0.91666667,  0.83333333,  1.        ,  1.        ,  0.83333333]),\n",
       "   'KFold13': array([ 0.72727273,  0.8       ,  0.9       ,  1.        ,  0.9       ,\n",
       "           0.7       ,  1.        ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold3': array([ 0.84090909,  0.90243902,  0.90243902]),\n",
       "   'KFold5': array([ 0.85185185,  0.88461538,  0.88      ,  0.91666667,  0.83333333]),\n",
       "   'KFold7': array([ 0.8       ,  0.88888889,  0.88888889,  0.88888889,  0.88888889,\n",
       "           1.        ,  0.88235294])},\n",
       "  'randomforest': {'KFold10': array([ 0.8       ,  0.85714286,  1.        ,  0.91666667,  0.66666667,\n",
       "           0.91666667,  1.        ,  1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold13': array([ 0.81818182,  0.9       ,  0.8       ,  1.        ,  0.9       ,\n",
       "           0.7       ,  0.8       ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold3': array([ 0.84090909,  0.90243902,  0.73170732]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.88      ,  1.        ,  0.83333333]),\n",
       "   'KFold7': array([ 0.8       ,  1.        ,  0.88888889,  0.88888889,  0.94444444,\n",
       "           0.94117647,  0.88235294])}},\n",
       " 'dtree': {'bagging': {'KFold10': array([ 0.8       ,  0.78571429,  0.92307692,  0.91666667,  0.75      ,\n",
       "           0.91666667,  0.91666667,  1.        ,  1.        ,  0.83333333]),\n",
       "   'KFold13': array([ 0.90909091,  0.7       ,  1.        ,  1.        ,  0.8       ,\n",
       "           0.7       ,  0.9       ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold3': array([ 0.90909091,  0.87804878,  0.82926829]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.88      ,  0.95833333,  0.95833333]),\n",
       "   'KFold7': array([ 0.8       ,  0.88888889,  0.94444444,  0.83333333,  0.88888889,\n",
       "           1.        ,  0.94117647])},\n",
       "  'dtree': {'KFold10': array([ 0.86666667,  0.85714286,  1.        ,  0.91666667,  0.75      ,\n",
       "           0.91666667,  0.91666667,  1.        ,  1.        ,  0.91666667]),\n",
       "   'KFold13': array([ 0.81818182,  0.7       ,  0.8       ,  0.9       ,  0.9       ,\n",
       "           0.7       ,  0.9       ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.875     ]),\n",
       "   'KFold3': array([ 0.90909091,  0.92682927,  0.85365854]),\n",
       "   'KFold5': array([ 0.85185185,  0.92307692,  0.88      ,  0.95833333,  0.79166667]),\n",
       "   'KFold7': array([ 0.8       ,  0.94444444,  0.94444444,  0.77777778,  0.88888889,\n",
       "           0.94117647,  0.88235294])},\n",
       "  'randomforest': {'KFold10': array([ 0.8       ,  0.85714286,  0.92307692,  0.83333333,  0.66666667,\n",
       "           0.91666667,  0.83333333,  1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold13': array([ 0.72727273,  0.9       ,  0.8       ,  0.9       ,  0.9       ,\n",
       "           0.7       ,  1.        ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.625     ]),\n",
       "   'KFold3': array([ 0.88636364,  0.92682927,  0.7804878 ]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.88      ,  0.91666667,  0.95833333]),\n",
       "   'KFold7': array([ 0.85      ,  0.88888889,  0.94444444,  0.94444444,  0.94444444,\n",
       "           1.        ,  0.94117647])}},\n",
       " 'randomforest': {'bagging': {'KFold10': array([ 0.93333333,  0.85714286,  1.        ,  0.91666667,  0.75      ,\n",
       "           0.91666667,  0.91666667,  1.        ,  1.        ,  0.83333333]),\n",
       "   'KFold13': array([ 0.72727273,  0.9       ,  0.9       ,  0.9       ,  0.9       ,\n",
       "           0.7       ,  0.9       ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold3': array([ 0.90909091,  0.92682927,  0.68292683]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.88      ,  0.91666667,  0.95833333]),\n",
       "   'KFold7': array([ 0.75      ,  0.94444444,  0.94444444,  0.88888889,  0.88888889,\n",
       "           1.        ,  0.94117647])},\n",
       "  'dtree': {'KFold10': array([ 0.86666667,  0.78571429,  0.92307692,  0.91666667,  0.75      ,\n",
       "           0.91666667,  1.        ,  1.        ,  1.        ,  0.75      ]),\n",
       "   'KFold13': array([ 0.81818182,  0.6       ,  0.9       ,  1.        ,  0.9       ,\n",
       "           0.7       ,  1.        ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  1.        ,  0.875     ]),\n",
       "   'KFold3': array([ 0.86363636,  0.92682927,  0.80487805]),\n",
       "   'KFold5': array([ 0.85185185,  0.92307692,  0.88      ,  0.95833333,  0.91666667]),\n",
       "   'KFold7': array([ 0.8       ,  0.88888889,  0.94444444,  0.88888889,  0.94444444,\n",
       "           1.        ,  0.88235294])},\n",
       "  'randomforest': {'KFold10': array([ 0.86666667,  0.92857143,  0.92307692,  0.91666667,  0.75      ,\n",
       "           0.91666667,  0.91666667,  1.        ,  0.83333333,  0.91666667]),\n",
       "   'KFold13': array([ 0.81818182,  0.8       ,  0.7       ,  1.        ,  0.9       ,\n",
       "           0.7       ,  1.        ,  0.9       ,  1.        ,  1.        ,\n",
       "           1.        ,  0.88888889,  0.875     ]),\n",
       "   'KFold3': array([ 0.90909091,  0.82926829,  0.87804878]),\n",
       "   'KFold5': array([ 0.85185185,  0.96153846,  0.88      ,  0.95833333,  0.91666667]),\n",
       "   'KFold7': array([ 0.8       ,  0.83333333,  0.83333333,  0.83333333,  0.88888889,\n",
       "           1.        ,  0.82352941])}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import sklearn.cross_validation as sklcv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "\"\"\"Each function returns a dictionary of observations and labels\"\"\"\n",
    "\n",
    "def load_reduced():\n",
    "    \"\"\"Load Reduced dataset. Return a dict\"\"\"\n",
    "    data = np.genfromtxt('data/DNA/ReducedDataSet_dataonly.csv')\n",
    "    X = np.transpose(data[1:, :-10])\n",
    "    Y = np.transpose(data[0, :-10])\n",
    "    dset = {}\n",
    "    dset['observations'] = X\n",
    "    dset['labels'] = Y\n",
    "    \n",
    "    return dset\n",
    "\n",
    "def load_leuk():\n",
    "    data = np.genfromtxt('data/DNA/labeled_leuk_corrected.csv',\n",
    "                        delimiter = ',', skip_header = 1)\n",
    "    X = data[:, 1:-1]\n",
    "    Y = data[:, -1]\n",
    "    dset = {}\n",
    "    dset['observations'] = X\n",
    "    dset['labels'] = Y\n",
    "    \n",
    "    return dset\n",
    "\n",
    "def load_bladder():\n",
    "    data = np.genfromtxt('data/DNA/labeled_bladder.csv',\n",
    "                        delimiter = ',', skip_header = 1)\n",
    "    X = data[:, 1:-1]\n",
    "    Y = data[:, -1]\n",
    "    dset = {}\n",
    "    dset['observations'] = X\n",
    "    dset['labels'] = Y\n",
    "    return dset\n",
    "\n",
    "def analyze_data(dataset):\n",
    "    \"\"\"Produce a dict structure of error and runtime values.\"\"\"\n",
    "    dsets = {'reduced': load_reduced,\n",
    "            'leuk': load_leuk,\n",
    "            'bladder': load_bladder}\n",
    "    dstf = dsets[dataset]\n",
    "    dset = dstf()\n",
    "    X = dset['observations']\n",
    "    Y = dset['labels']\n",
    "    num_obs = X.shape[0]\n",
    "    \n",
    "    results = {'dtree': {},\n",
    "              'bagging': {},\n",
    "              'randomforest': {},\n",
    "              'adaboost': {}}\n",
    "    \n",
    "    classifiers = {'dtree': skl.tree.DecisionTreeClassifier(),\n",
    "                   'bagging': BaggingClassifier(),\n",
    "                   'randomforest': RandomForestClassifier()}\n",
    "    cvs = {'KFold3': 3,\n",
    "          'KFold5': 5,\n",
    "          'KFold7': 7,\n",
    "          'KFold10': 10,\n",
    "          'KFold13': 13}\n",
    "    \n",
    "    for algorithm, value in results.iteritems():\n",
    "        errordt = {}\n",
    "        for key, clf in classifiers.iteritems():\n",
    "            print clf\n",
    "            classfd = {}\n",
    "            for cvkey, cvnum in cvs.iteritems():\n",
    "                C = skl.tree.DecisionTreeClassifier()\n",
    "                classfd[cvkey] = sklcv.cross_val_score(C, X, Y, cv=cvnum)\n",
    "            errordt[key] = classfd\n",
    "        results[algorithm] = errordt\n",
    "    \n",
    "    return results\n",
    "\n",
    "analyze_data('bladder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Example For CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================\n",
    "Gradient Boosting Out-of-Bag estimates\n",
    "======================================\n",
    "\n",
    "Out-of-bag (OOB) estimates can be a useful heuristic to estimate\n",
    "the \"optimal\" number of boosting iterations.\n",
    "OOB estimates are almost identical to cross-validation estimates but\n",
    "they can be computed on-the-fly without the need for repeated model\n",
    "fitting.\n",
    "OOB estimates are only available for Stochastic Gradient Boosting\n",
    "(i.e. ``subsample < 1.0``), the estimates are derived from the improvement\n",
    "in loss based on the examples not included in the bootstrap sample\n",
    "(the so-called out-of-bag examples).\n",
    "The OOB estimator is a pessimistic estimator of the true\n",
    "test loss, but remains a fairly good approximation for a small number of trees.\n",
    "\n",
    "The figure shows the cumulative sum of the negative OOB improvements\n",
    "as a function of the boosting iteration. As you can see, it tracks the test\n",
    "loss for the first hundred iterations but then diverges in a\n",
    "pessimistic way.\n",
    "The figure also shows the performance of 3-fold cross validation which\n",
    "usually gives a better estimate of the test loss\n",
    "but is computationally more demanding.\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "# Generate data (adapted from G. Ridgeway's gbm example)\n",
    "n_samples = 1000\n",
    "random_state = np.random.RandomState(13)\n",
    "x1 = random_state.uniform(size=n_samples)\n",
    "x2 = random_state.uniform(size=n_samples)\n",
    "x3 = random_state.randint(0, 4, size=n_samples)\n",
    "\n",
    "p = 1 / (1.0 + np.exp(-(np.sin(3 * x1) - 4 * x2 + x3)))\n",
    "y = random_state.binomial(1, p, size=n_samples)\n",
    "\n",
    "X = np.c_[x1, x2, x3]\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n",
    "                                                    random_state=9)\n",
    "\n",
    "# Fit classifier with out-of-bag estimates\n",
    "params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,\n",
    "          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(\"Accuracy: {:.4f}\".format(acc))\n",
    "\n",
    "n_estimators = params['n_estimators']\n",
    "x = np.arange(n_estimators) + 1\n",
    "\n",
    "\n",
    "def heldout_score(clf, X_test, y_test):\n",
    "    \"\"\"compute deviance scores on ``X_test`` and ``y_test``. \"\"\"\n",
    "    score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        score[i] = clf.loss_(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def cv_estimate(n_folds=3):\n",
    "    cv = KFold(n=X_train.shape[0], n_folds=n_folds)\n",
    "    cv_clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for train, test in cv:\n",
    "        cv_clf.fit(X_train[train], y_train[train])\n",
    "        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n",
    "    val_scores /= n_folds\n",
    "    return val_scores\n",
    "\n",
    "\n",
    "# Estimate best n_estimator using cross-validation\n",
    "cv_score = cv_estimate(3)\n",
    "\n",
    "# Compute best n_estimator for test data\n",
    "test_score = heldout_score(clf, X_test, y_test)\n",
    "\n",
    "# negative cumulative sum of oob improvements\n",
    "cumsum = -np.cumsum(clf.oob_improvement_)\n",
    "\n",
    "# min loss according to OOB\n",
    "oob_best_iter = x[np.argmin(cumsum)]\n",
    "\n",
    "# min loss according to test (normalize such that first loss is 0)\n",
    "test_score -= test_score[0]\n",
    "test_best_iter = x[np.argmin(test_score)]\n",
    "\n",
    "# min loss according to cv (normalize such that first loss is 0)\n",
    "cv_score -= cv_score[0]\n",
    "cv_best_iter = x[np.argmin(cv_score)]\n",
    "\n",
    "# color brew for the three curves\n",
    "oob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\n",
    "test_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\n",
    "cv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n",
    "\n",
    "# plot curves and vertical lines for best iterations\n",
    "plt.plot(x, cumsum, label='OOB loss', color=oob_color)\n",
    "plt.plot(x, test_score, label='Test loss', color=test_color)\n",
    "plt.plot(x, cv_score, label='CV loss', color=cv_color)\n",
    "plt.axvline(x=oob_best_iter, color=oob_color)\n",
    "plt.axvline(x=test_best_iter, color=test_color)\n",
    "plt.axvline(x=cv_best_iter, color=cv_color)\n",
    "\n",
    "# add three vertical lines to xticks\n",
    "xticks = plt.xticks()\n",
    "xticks_pos = np.array(xticks[0].tolist() +\n",
    "                      [oob_best_iter, cv_best_iter, test_best_iter])\n",
    "xticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +\n",
    "                        ['OOB', 'CV', 'Test'])\n",
    "ind = np.argsort(xticks_pos)\n",
    "xticks_pos = xticks_pos[ind]\n",
    "xticks_label = xticks_label[ind]\n",
    "plt.xticks(xticks_pos, xticks_label)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('normalized loss')\n",
    "plt.xlabel('number of iterations')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

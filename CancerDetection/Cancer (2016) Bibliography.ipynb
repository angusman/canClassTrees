{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Detection via Decision Trees using DNA Microarray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "Develop computational algorithms for cancer detection from gene expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Design\n",
    "1. Gene selection \n",
    "+ Weighting\n",
    "+ Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Bibliography\n",
    "\n",
    "### DNA Microarray Data\n",
    "[How to Analyze DNA Microarray Data, HHMI (2016 web)](http://media.hhmi.org/biointeractive/click/microarray_analyzing/01.html)\n",
    "* Microarray background and analysis fundamentals. Ratio of cancer-expressed to normal-expressed encoded as red-green color intensity. Log2 transform values (normal: intensity < 0 (green), even: intensity = 0 (black), cancer: intensity > 0 (red)). Hierarchical clustering with correlation metric to classify.\n",
    "\n",
    "[Pitfalls in the Use of DNA Microarray Data, Simon et. al (2003)](docs/Pitfalls in the Use of DNA Microarray Data 2003.pdf)\n",
    "* Very important critique of class prediction methods! Common errors: 1) using clustering for gene filtering (feature selection), 2) over-fitting (too many parameters, not enough observations). Recommendations: 1) supervised methods of filtering, 2) complete cross-validation of filtering, weighting, and predicting at every stage, 3) independent validation sets, 4) honest error reporting, 5) comparison to standard methods.\n",
    "\n",
    "[Using DNA Microarrays, Simon (2003)](doc/Using DNA Microarrays 2003.pdf)\n",
    "* Followup to the preceding. Terminology set.\n",
    "\n",
    "### Clustering (Classification & Gene Selection/Feature Selection/Filtering)\n",
    "[Hierarchical clustering, Baggerly & Broom (2009 slide)](docs/Analysis of Microarray Data 2009 slide.pdf)\n",
    "* Hierarchical clustering fundamentals and pitfalls. Agglomerative implementation (bottom-up) joining two closest samples into a cluster and repeating. Distance metrics (for individual vectors) and linkage rule (for generalizing individual distance metric). Clustering always finds clusters (even if no real meaning)! Validity can be tested by perturbing the data, e.g., bootsrap resampling (randomly sample gene rows with replacement, allowing for duplicates) and observing consistency. For data management, beware naive filtering which \"bends reality to your will.\" Consider instead, total variation, all genes on a given chromosome, all genes in a given ontology. \n",
    "\n",
    "[Clustering: Hierarchical & Partitioning, Sanchez (2011 slide)](docs/Clustering Methods for Class Discovery 2011 slide.pdf)\n",
    "* More mathematical treatment of the preceding. Also introduces clustering via partitioning with k-means method. Specify number of clusters (k), initialize cluster centers, then iteratively 1) assign each object to cluster with closest center, 2) take centroids of obtained clusters as new centers. Initializing cluster centers is problematic, usually done within a probability distribution framework.\n",
    "\n",
    "### Prediction\n",
    "[Prediction Error Estimation, Simon et. al (2005)](docs/predictionErrorEstimation.pdf)\n",
    "* Best results for error and bias: leave-one-out-cross-validation, and 10-fold cross-validation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
